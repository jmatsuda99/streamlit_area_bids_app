
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import sqlite3
from datetime import datetime, date

DB_PATH = "data.db"

st.set_page_config(
    page_title="ã‚¨ãƒªã‚¢åˆ¥å…¥æœ­ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# -----------------------------
# Fonts & CSS (Noto Sans JP)
# -----------------------------
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;600;700&display=swap');
html, body, [class*="css"]  {
  font-family: 'Noto Sans JP', sans-serif;
}
/* KPI cards */
.kpi {
  padding: 12px 16px;
  border-radius: 12px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0,0,0,0.03);
}
.small {
  font-size: 12px;
  color: #666;
}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“Š ã‚¨ãƒªã‚¢åˆ¥å…¥æœ­ãƒ‡ãƒ¼ã‚¿ å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

# -----------------------------
# DB helpers
# -----------------------------
def get_conn():
    return sqlite3.connect(DB_PATH, check_same_thread=False)

def init_db():
    with get_conn() as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS records (
                id INTEGER PRIMARY KEY AUTOINCREMENT
            );
        """)
        # ãƒ¡ã‚¿æƒ…å ±ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸ã‚“ã åˆ—åãªã©ã‚’ä¿ç®¡ã€å°†æ¥æ‹¡å¼µç”¨ï¼‰
        conn.execute("""
            CREATE TABLE IF NOT EXISTS meta (
                key TEXT PRIMARY KEY,
                val TEXT
            );
        """)

def table_exists(conn, table_name: str) -> bool:
    q = "SELECT name FROM sqlite_master WHERE type='table' AND name=?"
    return conn.execute(q, (table_name,)).fetchone() is not None

def num_rows() -> int:
    with get_conn() as conn:
        if not table_exists(conn, "records"):
            return 0
        # recordsãƒ†ãƒ¼ãƒ–ãƒ«ãŒç©ºã§ã‚‚åˆ—ãŒæ²¢å±±ã‚ã‚‹å¯èƒ½æ€§ã«æ³¨æ„
        try:
            return conn.execute("SELECT COUNT(1) FROM records").fetchone()[0]
        except:
            return 0

def infer_and_create_records_table(df: pd.DataFrame):
    """
    åˆå›å–ã‚Šè¾¼ã¿æ™‚ã«recordsãƒ†ãƒ¼ãƒ–ãƒ«ãŒãªã„å ´åˆã€dfã®åˆ—æ§‹æˆã‚’ã‚‚ã¨ã«å¯å¤‰ã‚¹ã‚­ãƒ¼ãƒã§ç”Ÿæˆã™ã‚‹ã€‚
    """
    with get_conn() as conn:
        # recordsãƒ†ãƒ¼ãƒ–ãƒ«ãŒç©ºãªã‚‰ã€ä¸€æ—¦å‰Šé™¤â†’ä½œã‚Šç›´ã—ï¼ˆåˆ—ã‚’æŸ”è»Ÿã«åæ˜ ï¼‰
        conn.execute("DROP TABLE IF EXISTS records;")
        # åˆ—åâ†’SQLåˆ—å®šç¾©ï¼ˆå…¨ã¦TEXTã§å–ã‚Šè¾¼ã¿ã€å¾Œæ®µã§å‹å¤‰æ›ï¼‰
        cols_sql = ",\n".join([f'"{c}" TEXT' for c in df.columns])
        conn.execute(f"""
            CREATE TABLE records (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                {cols_sql}
            );
        """)

def append_df(df: pd.DataFrame):
    """
    recordsã¸è¿½è¨˜ã€‚åˆ—ãŒä¸è¶³ã—ã¦ã„ã‚Œã°æ‹¡å¼µã€ä½™è¨ˆãªåˆ—ã¯ç„¡è¦–ã—ãªã„ï¼ˆæ‹¡å¼µã—ã¦å–ã‚Šè¾¼ã‚€ï¼‰
    """
    if df.empty:
        return 0
    with get_conn() as conn:
        # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ—ä¸€è¦§ã‚’å–å¾—
        cur_cols = []
        try:
            cur = conn.execute("PRAGMA table_info(records);").fetchall()
            cur_cols = [c[1] for c in cur]  # [cid, name, type, notnull, dflt, pk]
        except:
            pass

        # æœ€åˆã®å–ã‚Šè¾¼ã¿ or ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæœªå®šç¾©ã®æ™‚ã¯ä½œã‚Šç›´ã—
        if not cur_cols or len(cur_cols) <= 1:  # idã®ã¿ç­‰
            infer_and_create_records_table(df)
            cur_cols = [c[1] for c in conn.execute("PRAGMA table_info(records);").fetchall()]

        # ä¸è¶³ã—ã¦ã„ã‚‹åˆ—ãŒã‚ã‚Œã°è¿½åŠ ï¼ˆTEXTå‹ï¼‰
        add_cols = [c for c in df.columns if c not in cur_cols]
        for c in add_cols:
            try:
                conn.execute(f'ALTER TABLE records ADD COLUMN "{c}" TEXT;')
            except Exception as e:
                st.warning(f"åˆ—è¿½åŠ ã«å¤±æ•—: {c} -> {e}")

        # å–ã‚Šè¾¼ã¿
        # æ–‡å­—åˆ—åŒ–ã—ã¦ã‹ã‚‰INSERTï¼ˆå‹ã¯å¾Œã§é¸æŠå¯èƒ½ã«ï¼‰
        df_to_insert = df.copy()
        for c in df_to_insert.columns:
            df_to_insert[c] = df_to_insert[c].astype(str)
        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
        placeholders = ",".join(["?"] * len(df_to_insert.columns))
        colnames = ",".join([f'"{c}"' for c in df_to_insert.columns])
        conn.executemany(
            f'INSERT INTO records ({colnames}) VALUES ({placeholders})',
            df_to_insert.values.tolist()
        )
        conn.commit()
        return len(df_to_insert)

def load_excel_all_sheets(file, add_region_from_sheet=True, region_col_name="åœ°åŸŸ"):
    xls = pd.ExcelFile(file)
    frames = []
    for sheet in xls.sheet_names:
        df = pd.read_excel(xls, sheet_name=sheet)
        if add_region_from_sheet:
            df[region_col_name] = str(sheet)
        frames.append(df)
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()

def get_all_records_df(limit: int|None=None) -> pd.DataFrame:
    with get_conn() as conn:
        if not table_exists(conn, "records"):
            return pd.DataFrame()
        q = "SELECT * FROM records ORDER BY id"
        if limit and limit > 0:
            q += f" LIMIT {int(limit)}"
        df = pd.read_sql_query(q, conn)
    # idåˆ—ã¯å†…éƒ¨ç”¨
    if "id" in df.columns:
        df = df.drop(columns=["id"])
    return df

def reset_db():
    with get_conn() as conn:
        conn.execute("DROP TABLE IF EXISTS records;")
        conn.execute("DROP TABLE IF EXISTS meta;")
    init_db()

# -----------------------------
# Init
# -----------------------------
init_db()

with st.sidebar:
    st.subheader("âš™ï¸ ãƒ‡ãƒ¼ã‚¿ç®¡ç†")
    st.caption("Excelï¼ˆè¤‡æ•°ã‚·ãƒ¼ãƒˆå¯ï¼‰ã‚’å–ã‚Šè¾¼ã¿ã€å†…éƒ¨DB(SQLite)ã«ä¿å­˜ã—ã¾ã™ã€‚ä»¥å¾Œã¯DBã‹ã‚‰é«˜é€Ÿã«å¯è¦–åŒ–ã§ãã¾ã™ã€‚")
    uploaded = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆ.xlsxï¼‰", type=["xlsx"], accept_multiple_files=True)
    add_region = st.checkbox("ã‚·ãƒ¼ãƒˆåã‚’åœ°åŸŸåã¨ã—ã¦ä»˜ä¸ã™ã‚‹", value=True)
    region_col_name = st.text_input("åœ°åŸŸåˆ—ã®åˆ—å", value="åœ°åŸŸ")
    col1, col2 = st.columns(2)
    with col1:
        go = st.button("ğŸ“¥ å–ã‚Šè¾¼ã¿/è¿½åŠ ")
    with col2:
        clear = st.button("ğŸ—‘ï¸ DBãƒªã‚»ãƒƒãƒˆï¼ˆå…¨å‰Šé™¤ï¼‰", type="secondary")

    if clear:
        reset_db()
        st.success("DBã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

    if go and uploaded:
        total = 0
        for up in uploaded:
            try:
                df = load_excel_all_sheets(up, add_region_from_sheet=add_region, region_col_name=region_col_name)
                total += append_df(df)
            except Exception as e:
                st.error(f"{up.name} ã®å–ã‚Šè¾¼ã¿ã«å¤±æ•—: {e}")
        st.success(f"å–ã‚Šè¾¼ã¿å®Œäº†: {total} è¡Œã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")

    st.write("---")
    st.caption("ğŸ“¦ ç¾åœ¨ã®DBä»¶æ•°")
    st.metric(label="ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", value=f"{num_rows():,}")

# å–ã‚Šè¾¼ã¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
raw_df = get_all_records_df()

if raw_df.empty:
    st.info("ã¾ãšã¯å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Excelã‚’å–ã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚è¤‡æ•°ã‚·ãƒ¼ãƒˆã¯è‡ªå‹•ã§ç¸¦çµåˆã•ã‚Œã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚·ãƒ¼ãƒˆåâ†’åœ°åŸŸåˆ—ã‚’ä»˜ä¸ã§ãã¾ã™ã€‚")
    st.stop()

# -----------------------------
# åˆ—ã®å½¹å‰²ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šï¼ˆæŸ”è»Ÿå¯¾å¿œï¼‰
# -----------------------------
st.subheader("ğŸ” åˆ—ã®å½¹å‰²è¨­å®š")
cols = list(raw_df.columns)
# æ¨å®šå€™è£œ
date_cand = [c for c in cols if any(k in c.lower() for k in ["date", "æ—¥ä»˜", "æ—¥æ™‚", "time", "æ™‚åˆ»", "é–‹å§‹", "å–å¼•"])]
region_cand = [c for c in cols if any(k in c for k in ["åœ°åŸŸ","ã‚¨ãƒªã‚¢","ã‚¨ãƒªã‚¢å","ä¾›çµ¦ã‚¨ãƒªã‚¢","ã‚¨ãƒªã‚¢ã‚³ãƒ¼ãƒ‰","ã‚¨ãƒªã‚¢å","area","region","åœ°åŸŸå"])]
numeric_cand = [c for c in cols if c not in date_cand and c not in region_cand]

c1, c2, c3 = st.columns([1,1,2])
with c1:
    date_col = st.selectbox("æ—¥ä»˜åˆ—", options=cols, index=(cols.index(date_cand[0]) if date_cand else 0))
with c2:
    region_col = st.selectbox("åœ°åŸŸåˆ—", options=cols, index=(cols.index(region_cand[0]) if region_cand else cols.index(cols[-1])))
with c3:
    metric_cols = st.multiselect("æ•°å€¤åˆ—ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰", options=cols, default=[c for c in numeric_cand][:3])

# å‹å¤‰æ›
df = raw_df.copy()
# æ—¥ä»˜
try:
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
except:
    pass
# æ•°å€¤åˆ—ï¼ˆé¸æŠåˆ—ã®ã¿ï¼‰
for m in metric_cols:
    # æ•°å€¤ã«å¤‰æ›ï¼ˆã‚«ãƒ³ãƒç­‰é™¤å»ï¼‰
    df[m] = pd.to_numeric(df[m].astype(str).str.replace(",", ""), errors="coerce")

# -----------------------------
# ãƒ•ã‚£ãƒ«ã‚¿
# -----------------------------
st.subheader("ğŸ§° ãƒ•ã‚£ãƒ«ã‚¿")
# åœ°åŸŸ
regions = sorted([x for x in df[region_col].dropna().astype(str).unique().tolist()])
sel_regions = st.multiselect("åœ°åŸŸã‚’é¸æŠ", options=regions, default=regions)
# æœŸé–“
valid_dates = df[date_col].dropna()
if valid_dates.empty:
    st.warning("æ—¥ä»˜åˆ—ã«æœ‰åŠ¹ãªå€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥ä»˜åˆ—ã®æŒ‡å®šã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()
min_d, max_d = valid_dates.min().date(), valid_dates.max().date()
r = st.slider("æœŸé–“ã‚’æŒ‡å®š", min_value=min_d, max_value=max_d, value=(min_d, max_d))
freq = st.selectbox("é›†è¨ˆç²’åº¦", options=["æ—¥æ¬¡","é€±æ¬¡","æœˆæ¬¡"], index=2)
agg_mode = st.selectbox("é›†è¨ˆæ–¹æ³•", options=["å¹³å‡","åˆè¨ˆ","ä¸­å¤®å€¤"], index=0)

# ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
mask = (df[region_col].astype(str).isin(sel_regions)) & (df[date_col].dt.date.between(r[0], r[1]))
fdf = df.loc[mask].copy()

if fdf.empty:
    st.warning("æ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# -----------------------------
# é›†è¨ˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------
def resample_frame(frame: pd.DataFrame, on: str, by_region: bool, metrics: list[str], freq: str, how: str):
    tmp = frame[[on, region_col] + metrics].dropna(subset=[on]).copy()
    tmp = tmp.sort_values(on)
    # å‘¨æ³¢æ•°
    rule = {"æ—¥æ¬¡":"D","é€±æ¬¡":"W","æœˆæ¬¡":"MS"}[freq]
    # é›†è¨ˆé–¢æ•°
    agg_map = {"å¹³å‡":"mean","åˆè¨ˆ":"sum","ä¸­å¤®å€¤":"median"}[how]
    # æ™‚ç³»åˆ—ã«ã™ã‚‹ãŸã‚set_index
    tmp = tmp.set_index(on)
    if by_region:
        grp = tmp.groupby(region_col)
        out = []
        for g, gdf in grp:
            agg = gdf.resample(rule).agg(agg_map)
            agg[region_col] = g
            out.append(agg.reset_index())
        res = pd.concat(out, ignore_index=True)
    else:
        res = tmp.resample(rule).agg(agg_map).reset_index()
    return res

# -----------------------------
# KPI
# -----------------------------
st.subheader("ğŸ“ˆ æ¦‚è¦KPI")
kc1, kc2, kc3, kc4 = st.columns(4)
with kc1:
    st.markdown('<div class="kpi">ç·ä»¶æ•°<br><span class="small"></span><h3>{:,}</h3></div>'.format(len(fdf)), unsafe_allow_html=True)
with kc2:
    st.markdown('<div class="kpi">æœŸé–“<br><h3>{} ï½ {}</h3></div>'.format(r[0].strftime("%Y-%m-%d"), r[1].strftime("%Y-%m-%d")), unsafe_allow_html=True)
with kc3:
    st.markdown('<div class="kpi">åœ°åŸŸæ•°<br><h3>{}</h3></div>'.format(len(sel_regions)), unsafe_allow_html=True)
with kc4:
    st.markdown('<div class="kpi">é¸æŠæŒ‡æ¨™<br><h3>{}</h3></div>'.format(len(metric_cols)), unsafe_allow_html=True)

# -----------------------------
# å¯è¦–åŒ–
# -----------------------------
st.subheader("ğŸ“Š å¯è¦–åŒ–")

# (A) æ™‚ç³»åˆ—ï¼ˆåœ°åŸŸåˆ¥ï¼‰
if metric_cols:
    for m in metric_cols:
        st.markdown(f"**æ™‚ç³»åˆ—ï¼ˆ{m}ï¼‰**")
        ts = resample_frame(fdf, on=date_col, by_region=True, metrics=[m], freq=freq, how=agg_mode)
        # Altair
        chart = alt.Chart(ts).mark_line(point=True).encode(
            x=alt.X(f"{date_col}:T", title="æ—¥æ™‚"),
            y=alt.Y(f"{m}:Q", title=m),
            color=alt.Color(f"{region_col}:N", title="åœ°åŸŸ"),
            tooltip=[date_col, region_col, m]
        ).properties(height=300)
        st.altair_chart(chart, use_container_width=True)

# (B) åœ°åŸŸæ¨ªæ¯”è¼ƒï¼ˆå¹³å‡/åˆè¨ˆ/ä¸­å¤®å€¤ï¼‰
if metric_cols:
    st.markdown("**åœ°åŸŸæ¯”è¼ƒï¼ˆæœŸé–“å†…ã®é›†è¨ˆå€¤ï¼‰**")
    agg_func = {"å¹³å‡":"mean","åˆè¨ˆ":"sum","ä¸­å¤®å€¤":"median"}[agg_mode]
    comp = fdf.groupby(region_col)[metric_cols].agg(agg_func).reset_index()
    chart = alt.Chart(comp.melt(id_vars=[region_col], var_name="é …ç›®", value_name="å€¤")).mark_bar().encode(
        x=alt.X("é …ç›®:N", title="é …ç›®"),
        y=alt.Y("å€¤:Q", title="å€¤"),
        color=alt.Color(f"{region_col}:N", title="åœ°åŸŸ"),
        column=alt.Column(f"{region_col}:N", title="åœ°åŸŸ")
    ).properties(height=280)
    st.altair_chart(chart, use_container_width=True)

# (C) åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰
if metric_cols:
    st.markdown("**åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰**")
    m = st.selectbox("ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®å¯¾è±¡åˆ—", options=metric_cols, index=0)
    # ã‚¯ãƒªãƒƒãƒ—ã—ã¦æ¥µç«¯ãªå¤–ã‚Œå€¤ã‚’è»½æ¸›ï¼ˆ5~95%ï¼‰
    series = fdf[m].dropna()
    if len(series) > 0:
        q5, q95 = np.nanpercentile(series, 5), np.nanpercentile(series, 95)
        hist_df = pd.DataFrame({m: series.clip(q5, q95)})
        chart = alt.Chart(hist_df).mark_bar().encode(
            x=alt.X(f"{m}:Q", bin=alt.Bin(maxbins=40), title=m),
            y=alt.Y("count():Q", title="ä»¶æ•°")
        ).properties(height=260)
        st.altair_chart(chart, use_container_width=True)
    else:
        st.info("ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¯¾è±¡ã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

# (D) ãƒ”ãƒœãƒƒãƒˆï¼ˆåœ°åŸŸ Ã— æœˆã®ãƒãƒˆãƒªã‚¯ã‚¹ï¼‰
if metric_cols:
    st.markdown("**ãƒ”ãƒœãƒƒãƒˆï¼ˆåœ°åŸŸ Ã— æœˆï¼‰**")
    m = st.selectbox("ãƒ”ãƒœãƒƒãƒˆè¡¨ç¤ºã®å¯¾è±¡åˆ—", options=metric_cols, index=0, key="pivot_metric")
    tmp = fdf[[date_col, region_col, m]].dropna(subset=[date_col]).copy()
    tmp["æœˆ"] = tmp[date_col].dt.to_period("M").dt.to_timestamp()
    import numpy as _np
    agg = tmp.pivot_table(index=region_col, columns="æœˆ", values=m, aggfunc=_np.mean)
    st.dataframe(agg.style.format("{:,.2f}"), use_container_width=True)

# -----------------------------
# ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›
# -----------------------------
st.subheader("â¬‡ï¸ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›")
colx, coly = st.columns(2)
with colx:
    st.download_button(
        "ãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒ‡ãƒ¼ã‚¿ï¼ˆCSVï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=fdf.to_csv(index=False).encode("utf-8-sig"),
        file_name="filtered.csv",
        mime="text/csv"
    )
with coly:
    # é›†è¨ˆä¾‹ï¼šæœˆæ¬¡Ã—åœ°åŸŸÃ—å„æŒ‡æ¨™ï¼ˆå¹³å‡ï¼‰
    if metric_cols:
        t2 = fdf.copy()
        t2["æœˆ"] = t2[date_col].dt.to_period("M").dt.to_timestamp()
        out = t2.groupby(["æœˆ", region_col])[metric_cols].mean().reset_index().sort_values(["æœˆ", region_col])
        st.download_button(
            "æœˆæ¬¡é›†è¨ˆï¼ˆå¹³å‡, CSVï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=out.to_csv(index=False).encode("utf-8-sig"),
            file_name="monthly_summary_mean.csv",
            mime="text/csv"
        )

st.caption("Â© Streamlit app template for area bids by region (JP).")
