
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import sqlite3
import json
import re
from datetime import datetime, date, time
from typing import Optional

DB_PATH = "data.db"
RENAME_JSON = "rename_config.json"

st.set_page_config(
    page_title="ã‚¨ãƒªã‚¢åˆ¥å…¥æœ­ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸ“Š ã‚¨ãƒªã‚¢åˆ¥å…¥æœ­ãƒ‡ãƒ¼ã‚¿ å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

# -----------------------------
# DB helpers
# -----------------------------
def get_conn():
    return sqlite3.connect(DB_PATH, check_same_thread=False)

def init_db():
    with get_conn() as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS records (
                id INTEGER PRIMARY KEY AUTOINCREMENT
            );
        """)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS meta (
                key TEXT PRIMARY KEY,
                val TEXT
            );
        """)

def table_exists(conn, table_name: str) -> bool:
    q = "SELECT name FROM sqlite_master WHERE type='table' AND name=?"
    return conn.execute(q, (table_name,)).fetchone() is not None

def num_rows() -> int:
    with get_conn() as conn:
        if not table_exists(conn, "records"):
            return 0
        try:
            return conn.execute("SELECT COUNT(1) FROM records").fetchone()[0]
        except:
            return 0

def infer_and_create_records_table(df: pd.DataFrame):
    with get_conn() as conn:
        conn.execute("DROP TABLE IF EXISTS records;")
        cols_sql = ",\n".join([f'"{c}" TEXT' for c in df.columns])
        conn.execute(f"""
            CREATE TABLE records (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                {cols_sql}
            );
        """)

def append_df(df: pd.DataFrame):
    # å–ã‚Šè¾¼ã¿å‰ã«ãƒªãƒãƒ¼ãƒ è¨­å®šã‚’é©ç”¨
    _map = _load_rename_map()
    if _map:
        df = df.rename(columns=_map)

    if df.empty:
        return 0
    with get_conn() as conn:
        cur_cols = []
        try:
            cur = conn.execute("PRAGMA table_info(records);").fetchall()
            cur_cols = [c[1] for c in cur]
        except:
            pass

        if not cur_cols or len(cur_cols) <= 1:
            infer_and_create_records_table(df)
            cur_cols = [c[1] for c in conn.execute("PRAGMA table_info(records);").fetchall()]

        add_cols = [c for c in df.columns if c not in cur_cols]
        for c in add_cols:
            try:
                conn.execute(f'ALTER TABLE records ADD COLUMN "{c}" TEXT;')
            except Exception as e:
                st.warning(f"åˆ—è¿½åŠ ã«å¤±æ•—: {c} -> {e}")

        df_to_insert = df.copy()
        for c in df_to_insert.columns:
            df_to_insert[c] = df_to_insert[c].astype(str)

        placeholders = ",".join(["?"] * len(df_to_insert.columns))
        colnames = ",".join([f'"{c}"' for c in df_to_insert.columns])
        conn.executemany(
            f'INSERT INTO records ({colnames}) VALUES ({placeholders})',
            df_to_insert.values.tolist()
        )
        conn.commit()
        return len(df_to_insert)

def load_excel_all_sheets(file, add_region_from_sheet=True, region_col_name="åœ°åŸŸ"):
    xls = pd.ExcelFile(file)
    frames = []
    for sheet in xls.sheet_names:
        df = pd.read_excel(xls, sheet_name=sheet)
        if add_region_from_sheet:
            df[region_col_name] = str(sheet)
        frames.append(df)
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()

def get_all_records_df(limit: Optional[int]=None) -> pd.DataFrame:
    with get_conn() as conn:
        if not table_exists(conn, "records"):
            return pd.DataFrame()
        q = "SELECT * FROM records ORDER BY id"
        if limit and limit > 0:
            q += f" LIMIT {int(limit)}"
        df = pd.read_sql_query(q, conn)
    if "id" in df.columns:
        df = df.drop(columns=["id"])
    return df

def reset_db():
    with get_conn() as conn:
        conn.execute("DROP TABLE IF EXISTS records;")
        conn.execute("DROP TABLE IF EXISTS meta;")
    init_db()

# -----------------------------
# Rename helpers (DB-level)
# -----------------------------
def _sanitize_colname(name: str) -> str:
    if name is None:
        name = ""
    s = str(name).strip()
    s = re.sub(r"[\u3000\s]+", " ", s)
    return s if s else "col"

def _load_rename_map() -> dict:
    try:
        with open(RENAME_JSON, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _save_rename_map(mp: dict):
    with open(RENAME_JSON, "w", encoding="utf-8") as f:
        json.dump(mp, f, ensure_ascii=False, indent=2)

def _rebuild_db_with_mapping(mapping: dict):
    if not mapping:
        return 0
    tgt = [v for v in mapping.values()]
    norm = [str(v).strip() for v in tgt]
    if any(not x for x in norm):
        raise ValueError("ç©ºã®æ–°åˆ—åãŒã‚ã‚Šã¾ã™ã€‚ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚")
    if len(set(norm)) != len(norm):
        raise ValueError("æ–°ã—ã„åˆ—åãŒé‡è¤‡ã—ã¦ã„ã¾ã™ã€‚é‡è¤‡ã‚’è§£æ¶ˆã—ã¦ãã ã•ã„ã€‚")
    count = 0
    with get_conn() as conn:
        if not table_exists(conn, "records"):
            return 0
        cols_info = conn.execute("PRAGMA table_info(records);").fetchall()
        cur_cols = [c[1] for c in cols_info if c[1] != "id"]
        new_cols = [mapping.get(c, c) for c in cur_cols]

        conn.execute("DROP TABLE IF EXISTS records_new;")
        cols_sql = ", ".join([f'"{c}" TEXT' for c in new_cols])
        conn.execute(f'CREATE TABLE records_new (id INTEGER PRIMARY KEY AUTOINCREMENT, {cols_sql});')

        cols_list = ", ".join([f'"{c}"' for c in cur_cols])
        rows = conn.execute(f"SELECT {cols_list} FROM records").fetchall()
        for r in rows:
            row_dict = {mapping.get(col, col): (str(val) if val is not None else None) for col, val in zip(cur_cols, r)}
            new_cols_list = ", ".join([f'"{c}"' for c in new_cols])
            placeholders = ",".join(["?"] * len(new_cols))
            conn.execute(f"INSERT INTO records_new ({new_cols_list}) VALUES ({placeholders})", [row_dict[c] for c in new_cols])
            count += 1

        conn.execute("DROP TABLE IF EXISTS records;")
        conn.execute("ALTER TABLE records_new RENAME TO records;")
        conn.commit()
    return count

# -----------------------------
# Numeric-only hardening
# -----------------------------
def _coerce_numeric_columns(df_in: pd.DataFrame, cols):
    safe_cols = []
    dropped_cols = []
    if not cols:
        return df_in, [], []
    for c in cols:
        if c not in df_in.columns:
            continue
        ser = pd.to_numeric(df_in[c].astype(str).str.replace(",", ""), errors="coerce")
        if ser.notna().any():
            df_in[c] = ser
            safe_cols.append(c)
        else:
            dropped_cols.append(c)
    return df_in, safe_cols, dropped_cols

# -----------------------------
# Init
# -----------------------------
init_db()

with st.sidebar:
    st.subheader("âš™ï¸ ãƒ‡ãƒ¼ã‚¿ç®¡ç†")
    st.caption("Excelï¼ˆè¤‡æ•°ã‚·ãƒ¼ãƒˆå¯ï¼‰ã‚’å–ã‚Šè¾¼ã¿ã€å†…éƒ¨DB(SQLite)ã«ä¿å­˜ã—ã¾ã™ã€‚")

    uploaded = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆ.xlsxï¼‰", type=["xlsx"], accept_multiple_files=True)
    add_region = st.checkbox("ã‚·ãƒ¼ãƒˆåã‚’åœ°åŸŸåã¨ã—ã¦ä»˜ä¸ã™ã‚‹", value=True)
    region_col_name = st.text_input("åœ°åŸŸåˆ—ã®åˆ—å", value="åœ°åŸŸ")
    col1, col2 = st.columns(2)
    with col1:
        go = st.button("ğŸ“¥ å–ã‚Šè¾¼ã¿/è¿½åŠ ")
    with col2:
        clear = st.button("ğŸ—‘ï¸ DBãƒªã‚»ãƒƒãƒˆï¼ˆå…¨å‰Šé™¤ï¼‰")

    if clear:
        reset_db()
        st.success("DBã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

    if go and uploaded:
        total = 0
        for up in uploaded:
            try:
                df = load_excel_all_sheets(up, add_region_from_sheet=add_region, region_col_name=region_col_name)
                total += append_df(df)
            except Exception as e:
                st.error(f"{getattr(up, 'name', 'ãƒ•ã‚¡ã‚¤ãƒ«')} ã®å–ã‚Šè¾¼ã¿ã«å¤±æ•—: {e}")
        st.success(f"å–ã‚Šè¾¼ã¿å®Œäº†: {total} è¡Œã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")

    st.write("---")
    st.caption("ğŸ“¦ ç¾åœ¨ã®DBä»¶æ•°")
    st.metric(label="ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", value=f"{num_rows():,}")

    st.write("---")
    with st.expander("ğŸ“ åˆ—ãƒªãƒãƒ¼ãƒ ï¼ˆå†…éƒ¨DBã«é©ç”¨ï¼‰"):
        current_map = _load_rename_map()
        try:
            with get_conn() as _c:
                has_records = table_exists(_c, "records")
                cols_info = _c.execute("PRAGMA table_info(records);").fetchall() if has_records else []
                cur_cols = [c[1] for c in cols_info if c[1] != "id"]
        except Exception:
            has_records, cur_cols = False, []
        if not has_records or not cur_cols:
            st.caption("ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šè¾¼ã‚€ã¨ã€ã“ã“ã§åˆ—åã®ç½®ãæ›ãˆã‚’è¨­å®šã§ãã¾ã™ã€‚")
        else:
            st.caption("å·¦ãŒç¾åœ¨ã®åˆ—åã€‚å³ã«æ–°ã—ã„åˆ—åã‚’å…¥åŠ›ã—ã€ã€Œä¿å­˜ã€ã¾ãŸã¯ã€ŒDBã‚’å†æ§‹ç¯‰ã€ã‚’æŠ¼ã—ã¾ã™ã€‚")
            new_map = {}
            for c in cur_cols:
                default_alias = current_map.get(c, c)
                new_name = st.text_input(f"â†’ {c}", value=default_alias, key=f"rename_{c}")
                new_map[c] = _sanitize_colname(new_name)
            colA, colB = st.columns(2)
            with colA:
                if st.button("ğŸ’¾ ãƒªãƒãƒ¼ãƒ è¨­å®šã‚’ä¿å­˜"):
                    _save_rename_map(new_map)
                    st.success("ãƒªãƒãƒ¼ãƒ è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚æ¬¡å›å–ã‚Šè¾¼ã¿ã‚„å†æ§‹ç¯‰ã§æœ‰åŠ¹ã«ãªã‚Šã¾ã™ã€‚")
            with colB:
                if st.button("ğŸ› ï¸ DBã‚’ãƒªãƒãƒ¼ãƒ ã«åˆã‚ã›ã¦å†æ§‹ç¯‰"):
                    try:
                        _save_rename_map(new_map)
                        n = _rebuild_db_with_mapping(new_map)
                        st.success(f"DBã‚’å†æ§‹ç¯‰ã—ã¾ã—ãŸï¼ˆ{n:,} è¡Œç§»è¡Œï¼‰ã€‚ã‚¢ãƒ—ãƒªã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")
                    except Exception as e:
                        st.error(f"å†æ§‹ç¯‰ã«å¤±æ•—: {e}")

raw_df = get_all_records_df()
if raw_df.empty:
    st.info("ã¾ãšã¯å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Excelã‚’å–ã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚")
    st.stop()

# -----------------------------
# åˆ—ã®å½¹å‰²è¨­å®š
# -----------------------------
st.subheader("ğŸ” åˆ—ã®å½¹å‰²è¨­å®š")

cols = list(raw_df.columns)

# å„ªå…ˆ: 'ym' ã‚’æ—¥ä»˜å€™è£œã«å«ã‚ã‚‹ï¼ˆå…ˆé ­ã«ç½®ãï¼‰
date_cand = []
for c in cols:
    if c.lower() == "ym":
        date_cand.append(c)
        break
date_cand += [c for c in cols if any(k in c.lower() for k in ["date", "æ—¥ä»˜", "æ—¥æ™‚", "time", "æ™‚åˆ»", "é–‹å§‹", "å–å¼•"]) and c not in date_cand]

region_cand = [c for c in cols if any(k in c for k in ["åœ°åŸŸ","ã‚¨ãƒªã‚¢","ã‚¨ãƒªã‚¢å","ä¾›çµ¦ã‚¨ãƒªã‚¢","ã‚¨ãƒªã‚¢ã‚³ãƒ¼ãƒ‰","area","region","åœ°åŸŸå"])]
numeric_cand = [c for c in cols if c not in date_cand and c not in region_cand]

def _idx_or_default(opts, target):
    try:
        return opts.index(target)
    except:
        return 0

default_date_col = date_cand[0] if date_cand else cols[0]
default_region_col = region_cand[0] if region_cand else cols[-1]

c1, c2, c3 = st.columns([1,1,2])
with c1:
    date_col = st.selectbox("æ—¥ä»˜åˆ—", options=cols, index=_idx_or_default(cols, default_date_col))
with c2:
    region_col = st.selectbox("åœ°åŸŸåˆ—", options=cols, index=_idx_or_default(cols, default_region_col))
with c3:
    metric_cols = st.multiselect("æ•°å€¤åˆ—ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰", options=cols, default=[c for c in numeric_cand][:3])

# ---- Numeric-only hardening ----
df = raw_df.copy()
df, safe_metric_cols, dropped_metrics = _coerce_numeric_columns(df, metric_cols)
if dropped_metrics:
    st.warning("æ•°å€¤åŒ–ã§ããªã‹ã£ãŸåˆ—ã‚’é™¤å¤–ã—ã¾ã—ãŸ: " + ", ".join(dropped_metrics))

# -----------------------------
# ymã®3æ™‚é–“åˆ»ã¿å†æ§‹ç¯‰
# -----------------------------
st.markdown("**â±ï¸ ymåˆ—ã®æ™‚é–“å†æ§‹ç¯‰ï¼ˆ3æ™‚é–“åˆ»ã¿ï¼‰**")
rebuild = st.checkbox("å…ˆé ­ã‚’ 2024-04-01 00:00ã€ä»¥é™180åˆ†ãšã¤ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã§å†æ§‹ç¯‰ï¼ˆåœ°åŸŸã”ã¨ï¼‰", value=("ym" in [c.lower() for c in cols]))
start_date = st.date_input("é–‹å§‹æ—¥", value=date(2024,4,1))
start_time = st.time_input("é–‹å§‹æ™‚åˆ»", value=time(0,0))

if rebuild:
    start_dt = datetime.combine(start_date, start_time)
    try:
        df["_row_order_"] = np.arange(len(df))
        df[region_col] = df[region_col].astype(str)
        out = []
        for g, gdf in df.groupby(region_col, sort=False):
            gdf = gdf.sort_values("_row_order_").copy()
            rng = pd.date_range(start=start_dt, periods=len(gdf), freq="180min")
            gdf[date_col] = pd.to_datetime(rng)
            out.append(gdf)
        df = pd.concat(out, ignore_index=True).sort_values("_row_order_").drop(columns=["_row_order_"])
    except Exception as e:
        st.warning(f"ymå†æ§‹ç¯‰ã§ã‚¨ãƒ©ãƒ¼: {e}")

# Convert date and metrics
try:
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
except Exception as e:
    st.warning(f"æ—¥ä»˜å¤‰æ›ã§ã‚¨ãƒ©ãƒ¼: {e}")

# -----------------------------
# ãƒ•ã‚£ãƒ«ã‚¿
# -----------------------------
st.subheader("ğŸ§° ãƒ•ã‚£ãƒ«ã‚¿")
regions = sorted([x for x in df[region_col].dropna().astype(str).unique().tolist()])
sel_regions = st.multiselect("åœ°åŸŸã‚’é¸æŠ", options=regions, default=regions)

valid_dates = df[date_col].dropna()
if valid_dates.empty:
    st.warning("æ—¥ä»˜åˆ—ã«æœ‰åŠ¹ãªå€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥ä»˜åˆ—ã®æŒ‡å®šã‚„å†æ§‹ç¯‰è¨­å®šã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()
min_d, max_d = valid_dates.min().date(), valid_dates.max().date()
if min_d > max_d:
    min_d, max_d = max_d, min_d
r = st.slider("æœŸé–“ã‚’æŒ‡å®š", min_value=min_d, max_value=max_d, value=(min_d, max_d))
freq = st.selectbox("é›†è¨ˆç²’åº¦", options=["ç”Ÿãƒ‡ãƒ¼ã‚¿(180åˆ†)","æ—¥æ¬¡","é€±æ¬¡","æœˆæ¬¡"], index=1)
agg_mode = st.selectbox("é›†è¨ˆæ–¹æ³•", options=["å¹³å‡","åˆè¨ˆ","ä¸­å¤®å€¤"], index=0)

mask = (df[region_col].astype(str).isin(sel_regions)) & (df[date_col].dt.date.between(r[0], r[1]))
fdf = df.loc[mask].copy()

if fdf.empty:
    st.warning("æ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# -----------------------------
# é›†è¨ˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------
def resample_frame(frame: pd.DataFrame, on: str, by_region: bool, metrics: list, freq_label: str, how: str):
    tmp = frame[[on, region_col] + metrics].dropna(subset=[on]).copy()
    tmp = tmp.sort_values(on)
    rule_map = {"ç”Ÿãƒ‡ãƒ¼ã‚¿(180åˆ†)": "RAW", "æ—¥æ¬¡": "D", "é€±æ¬¡": "W", "æœˆæ¬¡": "MS"}
    rule = rule_map.get(freq_label, "MS")
    agg_map = {"å¹³å‡": "mean", "åˆè¨ˆ": "sum", "ä¸­å¤®å€¤": "median"}[how]

    tmp = tmp.set_index(on)
    for m in metrics:
        tmp[m] = pd.to_numeric(tmp[m], errors="coerce")

    if rule == "RAW":
        if by_region:
            return tmp.reset_index()[[on, region_col] + metrics]
        return tmp.reset_index()[[on] + metrics]

    if by_region:
        out = []
        for g, gdf in tmp.groupby(region_col):
            num = gdf[metrics].resample(rule).agg(agg_map)
            num[region_col] = g
            num = num.reset_index()
            out.append(num)
        return pd.concat(out, ignore_index=True)
    else:
        return tmp[metrics].resample(rule).agg(agg_map).reset_index()

# -----------------------------
# KPI
# -----------------------------
st.subheader("ğŸ“ˆ æ¦‚è¦KPI")
kc1, kc2, kc3, kc4 = st.columns(4)
with kc1:
    st.metric("ç·ä»¶æ•°", f"{len(fdf):,}")
with kc2:
    st.metric("æœŸé–“", f"{r[0].strftime('%Y-%m-%d')} ï½ {r[1].strftime('%Y-%m-%d')}")
with kc3:
    st.metric("åœ°åŸŸæ•°", f"{len(sel_regions)}")
with kc4:
    st.metric("é¸æŠæŒ‡æ¨™æ•°", f"{len(metric_cols)}")

# -----------------------------
# å¯è¦–åŒ–
# -----------------------------
st.subheader("ğŸ“Š å¯è¦–åŒ–")

# æ¯”ç‡ï¼ˆåˆ†å­/åˆ†æ¯ï¼‰
st.subheader("ğŸ§® æ¯”ç‡ï¼ˆåˆ†å­/åˆ†æ¯ï¼‰")
if safe_metric_cols:
    cnum, cden = st.columns(2)
    with cnum:
        ratio_num = st.selectbox("åˆ†å­ï¼ˆç³»åˆ—ï¼‰", options=safe_metric_cols, index=0, key="ratio_num")
    with cden:
        den_opts = [c for c in safe_metric_cols if c != ratio_num] or safe_metric_cols
        ratio_den = st.selectbox("åˆ†æ¯ï¼ˆç³»åˆ—ï¼‰", options=den_opts, index=0, key="ratio_den")

    def _compute_ratio(frame, num_col, den_col):
        out = frame[[date_col, region_col, num_col, den_col]].copy()
        out[num_col] = pd.to_numeric(out[num_col], errors="coerce")
        out[den_col] = pd.to_numeric(out[den_col], errors="coerce")
        out = out[out[den_col].notna() & (out[den_col] != 0)]
        out["__ratio__"] = out[num_col] / out[den_col]
        return out[[date_col, region_col, "__ratio__"]]

    rs = resample_frame(fdf, on=date_col, by_region=True, metrics=[ratio_num, ratio_den], freq_label=freq, how=agg_mode)
    rs_ratio = _compute_ratio(rs, ratio_num, ratio_den)

    chart_ratio = alt.Chart(rs_ratio).mark_line(point=True).encode(
        x=alt.X(f"{date_col}:T", title="æ—¥æ™‚"),
        y=alt.Y("__ratio__:Q", title="æ¯”ç‡", axis=alt.Axis(format='%')),
        color=alt.Color(f"{region_col}:N", title="åœ°åŸŸ"),
        tooltip=[date_col, region_col, alt.Tooltip("__ratio__:Q", title="æ¯”ç‡", format=".1%")]
    ).properties(height=300)
    st.altair_chart(chart_ratio, use_container_width=True)

    st.markdown("**åœ°åŸŸæ¯”è¼ƒï¼ˆÎ£åˆ†å­/Î£åˆ†æ¯ï¼‰**")
    grp = fdf.groupby(region_col, dropna=True)
    comp_ratio = grp[ratio_num].sum(min_count=1) / grp[ratio_den].sum(min_count=1)
    comp_ratio = comp_ratio.reset_index(name="æ¯”ç‡").dropna(subset=["æ¯”ç‡"])
    chart_comp_ratio = alt.Chart(comp_ratio).mark_bar().encode(
        x=alt.X(f"{region_col}:N", title="åœ°åŸŸ"),
        y=alt.Y("æ¯”ç‡:Q", title="æ¯”ç‡", axis=alt.Axis(format='%')),
        color=alt.Color(f"{region_col}:N", title="åœ°åŸŸ"),
        tooltip=[region_col, alt.Tooltip("æ¯”ç‡:Q", title="æ¯”ç‡", format=".1%")]
    ).properties(height=320)
    st.altair_chart(chart_comp_ratio, use_container_width=True)

    st.download_button(
        "æ¯”ç‡ã®æ™‚ç³»åˆ—ï¼ˆCSVï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=rs_ratio.rename(columns={"__ratio__": "ratio"}).to_csv(index=False).encode("utf-8-sig"),
        file_name="ratio_timeseries.csv",
        mime="text/csv"
    )
else:
    st.info("æ¯”ç‡è¨ˆç®—ã«ã¯æ•°å€¤åˆ—ãŒå¿…è¦ã§ã™ã€‚ã¾ãšã¯æ•°å€¤åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

# æ™‚ç³»åˆ—ï¼ˆå„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
if safe_metric_cols:
    for m in safe_metric_cols:
        st.markdown(f"**æ™‚ç³»åˆ—ï¼ˆ{m}ï¼‰**")
        ts = resample_frame(fdf, on=date_col, by_region=True, metrics=[m], freq_label=freq, how=agg_mode)
        chart = alt.Chart(ts).mark_line(point=True).encode(
            x=alt.X(f"{date_col}:T", title="æ—¥æ™‚"),
            y=alt.Y(f"{m}:Q", title=m),
            color=alt.Color(f"{region_col}:N", title="åœ°åŸŸ"),
            tooltip=[date_col, region_col, m]
        ).properties(height=300)
        st.altair_chart(chart, use_container_width=True)

    # åœ°åŸŸæ¯”è¼ƒï¼ˆæœŸé–“å†…ã®é›†è¨ˆå€¤ï¼‰: ã‚°ãƒ«ãƒ¼ãƒ—åŒ–æ£’
    st.markdown("**åœ°åŸŸæ¯”è¼ƒï¼ˆæœŸé–“å†…ã®é›†è¨ˆå€¤ï¼‰**")
    agg_func = {"å¹³å‡":"mean","åˆè¨ˆ":"sum","ä¸­å¤®å€¤":"median"}[agg_mode]
    comp = fdf.groupby(region_col)[safe_metric_cols].agg(agg_func).reset_index()
    melted = comp.melt(id_vars=[region_col], var_name="é …ç›®", value_name="å€¤")
    chart = alt.Chart(melted).mark_bar().encode(
        x=alt.X("é …ç›®:N", title="é …ç›®"),
        xOffset=alt.XOffset(f"{region_col}:N"),
        y=alt.Y("å€¤:Q", title="å€¤"),
        color=alt.Color(f"{region_col}:N", title="åœ°åŸŸ"),
        tooltip=["é …ç›®", region_col, "å€¤"]
    ).properties(height=320)
    st.altair_chart(chart, use_container_width=True)

    # åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰
    st.markdown("**åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰**")
    m = st.selectbox("ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®å¯¾è±¡åˆ—", options=safe_metric_cols, index=0)
    series = fdf[m].dropna()
    if len(series) > 0:
        q5, q95 = np.nanpercentile(series, 5), np.nanpercentile(series, 95)
        hist_df = pd.DataFrame({m: series.clip(q5, q95)})
        chart = alt.Chart(hist_df).mark_bar().encode(
            x=alt.X(f"{m}:Q", bin=alt.Bin(maxbins=40), title=m),
            y=alt.Y("count():Q", title="ä»¶æ•°")
        ).properties(height=260)
        st.altair_chart(chart, use_container_width=True)
    else:
        st.info("ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¯¾è±¡ã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    # ãƒ”ãƒœãƒƒãƒˆï¼ˆåœ°åŸŸ Ã— æœˆï¼‰
    st.markdown("**ãƒ”ãƒœãƒƒãƒˆï¼ˆåœ°åŸŸ Ã— æœˆï¼‰**")
    m = st.selectbox("ãƒ”ãƒœãƒƒãƒˆè¡¨ç¤ºã®å¯¾è±¡åˆ—", options=safe_metric_cols, index=0, key="pivot_metric")
    tmp = fdf[[date_col, region_col, m]].dropna(subset=[date_col]).copy()
    tmp["æœˆ"] = tmp[date_col].dt.to_period("M").dt.to_timestamp()
    agg = tmp.pivot_table(index=region_col, columns="æœˆ", values=m, aggfunc=np.mean)
    st.dataframe(agg.style.format("{:,.2f}"), use_container_width=True)

# -----------------------------
# ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›
# -----------------------------
st.subheader("â¬‡ï¸ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›")
colx, coly = st.columns(2)
with colx:
    st.download_button(
        "ãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒ‡ãƒ¼ã‚¿ï¼ˆCSVï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=fdf.to_csv(index=False).encode("utf-8-sig"),
        file_name="filtered.csv",
        mime="text/csv"
    )
with coly:
    if safe_metric_cols:
        t2 = fdf.copy()
        t2["æœˆ"] = t2[date_col].dt.to_period("M").dt.to_timestamp()
        out = t2.groupby(["æœˆ", region_col])[safe_metric_cols].mean().reset_index().sort_values(["æœˆ", region_col])
        st.download_button(
            "æœˆæ¬¡é›†è¨ˆï¼ˆå¹³å‡, CSVï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=out.to_csv(index=False).encode("utf-8-sig"),
            file_name="monthly_summary_mean.csv",
            mime="text/csv"
        )

st.caption("Â© Streamlit app template for area bids by region (JP).")
