
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import sqlite3
from datetime import datetime, date, time
from typing import Optional

DB_PATH = "data.db"

st.set_page_config(
    page_title="ã‚¨ãƒªã‚¢åˆ¥å…¥æœ­ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("ğŸ“Š ã‚¨ãƒªã‚¢åˆ¥å…¥æœ­ãƒ‡ãƒ¼ã‚¿ å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

# -----------------------------
# DB helpers
# -----------------------------
def get_conn():
    return sqlite3.connect(DB_PATH, check_same_thread=False)

def init_db():
    with get_conn() as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS records (
                id INTEGER PRIMARY KEY AUTOINCREMENT
            );
        """ )
        conn.execute("""
            CREATE TABLE IF NOT EXISTS meta (
                key TEXT PRIMARY KEY,
                val TEXT
            );
        """ )

def table_exists(conn, table_name: str) -> bool:
    q = "SELECT name FROM sqlite_master WHERE type='table' AND name=?"
    return conn.execute(q, (table_name,)).fetchone() is not None

def num_rows() -> int:
    with get_conn() as conn:
        if not table_exists(conn, "records"):
            return 0
        try:
            return conn.execute("SELECT COUNT(1) FROM records").fetchone()[0]
        except:
            return 0

def infer_and_create_records_table(df: pd.DataFrame):
    with get_conn() as conn:
        conn.execute("DROP TABLE IF EXISTS records;")
        cols_sql = ",\n".join([f'"{c}" TEXT' for c in df.columns])
        conn.execute(f"""
            CREATE TABLE records (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                {cols_sql}
            );
        """ )

def append_df(df: pd.DataFrame):
    if df.empty:
        return 0
    with get_conn() as conn:
        cur_cols = []
        try:
            cur = conn.execute("PRAGMA table_info(records);").fetchall()
            cur_cols = [c[1] for c in cur]
        except:
            pass

        if not cur_cols or len(cur_cols) <= 1:
            infer_and_create_records_table(df)
            cur_cols = [c[1] for c in conn.execute("PRAGMA table_info(records);").fetchall()]

        add_cols = [c for c in df.columns if c not in cur_cols]
        for c in add_cols:
            try:
                conn.execute(f'ALTER TABLE records ADD COLUMN "{c}" TEXT;')
            except Exception as e:
                st.warning(f"åˆ—è¿½åŠ ã«å¤±æ•—: {c} -> {e}")

        df_to_insert = df.copy()
        for c in df_to_insert.columns:
            df_to_insert[c] = df_to_insert[c].astype(str)

        placeholders = ",".join(["?"] * len(df_to_insert.columns))
        colnames = ",".join([f'"{c}"' for c in df_to_insert.columns])
        conn.executemany(
            f'INSERT INTO records ({colnames}) VALUES ({placeholders})',
            df_to_insert.values.tolist()
        )
        conn.commit()
        return len(df_to_insert)

def load_excel_all_sheets(file, add_region_from_sheet=True, region_col_name="åœ°åŸŸ"):
    xls = pd.ExcelFile(file)
    frames = []
    for sheet in xls.sheet_names:
        df = pd.read_excel(xls, sheet_name=sheet)
        if add_region_from_sheet:
            df[region_col_name] = str(sheet)
        frames.append(df)
    if frames:
        return pd.concat(frames, ignore_index=True)
    return pd.DataFrame()

def get_all_records_df(limit: Optional[int]=None) -> pd.DataFrame:
    with get_conn() as conn:
        if not table_exists(conn, "records"):
            return pd.DataFrame()
        q = "SELECT * FROM records ORDER BY id"
        if limit and limit > 0:
            q += f" LIMIT {int(limit)}"
        df = pd.read_sql_query(q, conn)
    if "id" in df.columns:
        df = df.drop(columns=["id"])
    return df

def reset_db():
    with get_conn() as conn:
        conn.execute("DROP TABLE IF EXISTS records;")
        conn.execute("DROP TABLE IF EXISTS meta;")
    init_db()

# -----------------------------
# Init
# -----------------------------
init_db()

with st.sidebar:
    st.subheader("âš™ï¸ ãƒ‡ãƒ¼ã‚¿ç®¡ç†")
    st.caption("Excelï¼ˆè¤‡æ•°ã‚·ãƒ¼ãƒˆå¯ï¼‰ã‚’å–ã‚Šè¾¼ã¿ã€å†…éƒ¨DB(SQLite)ã«ä¿å­˜ã—ã¾ã™ã€‚ä»¥å¾Œã¯DBã‹ã‚‰é«˜é€Ÿã«å¯è¦–åŒ–ã§ãã¾ã™ã€‚")
    uploaded = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆ.xlsxï¼‰", type=["xlsx"], accept_multiple_files=True)
    add_region = st.checkbox("ã‚·ãƒ¼ãƒˆåã‚’åœ°åŸŸåã¨ã—ã¦ä»˜ä¸ã™ã‚‹", value=True)
    region_col_name = st.text_input("åœ°åŸŸåˆ—ã®åˆ—å", value="åœ°åŸŸ")
    col1, col2 = st.columns(2)
    with col1:
        go = st.button("ğŸ“¥ å–ã‚Šè¾¼ã¿/è¿½åŠ ")
    with col2:
        # Remove type='secondary' for compatibility
        clear = st.button("ğŸ—‘ï¸ DBãƒªã‚»ãƒƒãƒˆï¼ˆå…¨å‰Šé™¤ï¼‰")

    if clear:
        reset_db()
        st.success("DBã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

    if go and uploaded:
        total = 0
        for up in uploaded:
            try:
                df = load_excel_all_sheets(up, add_region_from_sheet=add_region, region_col_name=region_col_name)
                total += append_df(df)
            except Exception as e:
                st.error(f"{up.name} ã®å–ã‚Šè¾¼ã¿ã«å¤±æ•—: {e}")
        st.success(f"å–ã‚Šè¾¼ã¿å®Œäº†: {total} è¡Œã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")

    st.write("---")
    st.caption("ğŸ“¦ ç¾åœ¨ã®DBä»¶æ•°")
    st.metric(label="ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", value=f"{num_rows():,}")

raw_df = get_all_records_df()
if raw_df.empty:
    st.info("ã¾ãšã¯å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Excelã‚’å–ã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚")
    st.stop()

# -----------------------------
# åˆ—ã®å½¹å‰²è¨­å®š
# -----------------------------
st.subheader("ğŸ” åˆ—ã®å½¹å‰²è¨­å®š")

cols = list(raw_df.columns)

# å„ªå…ˆ: 'ym' ã‚’æ—¥ä»˜å€™è£œã«å«ã‚ã‚‹ï¼ˆå…ˆé ­ã«ç½®ãï¼‰
date_cand = []
for c in cols:
    if c.lower() == "ym":
        date_cand.append(c)
        break
date_cand += [c for c in cols if any(k in c.lower() for k in ["date", "æ—¥ä»˜", "æ—¥æ™‚", "time", "æ™‚åˆ»", "é–‹å§‹", "å–å¼•"]) and c not in date_cand]

region_cand = [c for c in cols if any(k in c for k in ["åœ°åŸŸ","ã‚¨ãƒªã‚¢","ã‚¨ãƒªã‚¢å","ä¾›çµ¦ã‚¨ãƒªã‚¢","ã‚¨ãƒªã‚¢ã‚³ãƒ¼ãƒ‰","area","region","åœ°åŸŸå"])]
numeric_cand = [c for c in cols if c not in date_cand and c not in region_cand]

def _idx_or_default(opts, target):
    try:
        return opts.index(target)
    except:
        return 0

default_date_col = date_cand[0] if date_cand else cols[0]
default_region_col = region_cand[0] if region_cand else cols[-1]

c1, c2, c3 = st.columns([1,1,2])
with c1:
    date_col = st.selectbox("æ—¥ä»˜åˆ—", options=cols, index=_idx_or_default(cols, default_date_col))
with c2:
    region_col = st.selectbox("åœ°åŸŸåˆ—", options=cols, index=_idx_or_default(cols, default_region_col))
with c3:
    metric_cols = st.multiselect("æ•°å€¤åˆ—ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰", options=cols, default=[c for c in numeric_cand][:3])

# è¿½åŠ æ©Ÿèƒ½ï¼šymåˆ—ã®3æ™‚é–“åˆ»ã¿å†æ§‹ç¯‰
st.markdown("**â±ï¸ ymåˆ—ã®æ™‚é–“å†æ§‹ç¯‰ï¼ˆ3æ™‚é–“åˆ»ã¿ï¼‰**")
rebuild = st.checkbox("å…ˆé ­ã‚’ 2024-04-01 00:00ã€ä»¥é™180åˆ†ãšã¤ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã§å†æ§‹ç¯‰ï¼ˆåœ°åŸŸã”ã¨ï¼‰", value=("ym" in [c.lower() for c in cols]))
start_date = st.date_input("é–‹å§‹æ—¥", value=date(2024,4,1))
start_time = st.time_input("é–‹å§‹æ™‚åˆ»", value=time(0,0))

df = raw_df.copy()

if rebuild:
    start_dt = datetime.combine(start_date, start_time)
    try:
        df["_row_order_"] = np.arange(len(df))
        df[region_col] = df[region_col].astype(str)
        out = []
        for g, gdf in df.groupby(region_col, sort=False):
            gdf = gdf.sort_values("_row_order_").copy()
            rng = pd.date_range(start=start_dt, periods=len(gdf), freq="180min")
            gdf[date_col] = pd.to_datetime(rng)
            out.append(gdf)
        df = pd.concat(out, ignore_index=True).sort_values("_row_order_").drop(columns=["_row_order_"])
    except Exception as e:
        st.warning(f"ymå†æ§‹ç¯‰ã§ã‚¨ãƒ©ãƒ¼: {e}")

try:
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
except Exception as e:
    st.warning(f"æ—¥ä»˜å¤‰æ›ã§ã‚¨ãƒ©ãƒ¼: {e}")
for m in metric_cols:
    df[m] = pd.to_numeric(df[m].astype(str).str.replace(",", ""), errors="coerce")

# -----------------------------
# ãƒ•ã‚£ãƒ«ã‚¿
# -----------------------------
st.subheader("ğŸ§° ãƒ•ã‚£ãƒ«ã‚¿")
regions = sorted([x for x in df[region_col].dropna().astype(str).unique().tolist()])
sel_regions = st.multiselect("åœ°åŸŸã‚’é¸æŠ", options=regions, default=regions)

valid_dates = df[date_col].dropna()
if valid_dates.empty:
    st.warning("æ—¥ä»˜åˆ—ã«æœ‰åŠ¹ãªå€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥ä»˜åˆ—ã®æŒ‡å®šã‚„å†æ§‹ç¯‰è¨­å®šã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()
min_d, max_d = valid_dates.min().date(), valid_dates.max().date()
if min_d > max_d:
    min_d, max_d = max_d, min_d
r = st.slider("æœŸé–“ã‚’æŒ‡å®š", min_value=min_d, max_value=max_d, value=(min_d, max_d))
freq = st.selectbox("é›†è¨ˆç²’åº¦", options=["ç”Ÿãƒ‡ãƒ¼ã‚¿(180åˆ†)","æ—¥æ¬¡","é€±æ¬¡","æœˆæ¬¡"], index=1)
agg_mode = st.selectbox("é›†è¨ˆæ–¹æ³•", options=["å¹³å‡","åˆè¨ˆ","ä¸­å¤®å€¤"], index=0)

mask = (df[region_col].astype(str).isin(sel_regions)) & (df[date_col].dt.date.between(r[0], r[1]))
fdf = df.loc[mask].copy()

if fdf.empty:
    st.warning("æ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# -----------------------------
# é›†è¨ˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------
def resample_frame(frame: pd.DataFrame, on: str, by_region: bool, metrics: list, freq: str, how: str):
    """
    Resample with selectable frequency. For 'ç”Ÿãƒ‡ãƒ¼ã‚¿(180åˆ†)', bypass aggregation and return raw 180min data.
    """
    tmp = frame[[on, region_col] + metrics].dropna(subset=[on]).copy()
    tmp = tmp.sort_values(on)
    # Map label -> pandas rule; RAW means no resample
    rule_map = {"ç”Ÿãƒ‡ãƒ¼ã‚¿(180åˆ†)": "RAW", "æ—¥æ¬¡": "D", "é€±æ¬¡": "W", "æœˆæ¬¡": "MS"}
    rule = rule_map.get(freq, "MS")
    agg_map = {"å¹³å‡":"mean","åˆè¨ˆ":"sum","ä¸­å¤®å€¤":"median"}[how]

    # Ensure datetime index
    tmp = tmp.set_index(on)

    # Cast metric columns to numeric again (safety)
    for m in metrics:
        tmp[m] = pd.to_numeric(tmp[m], errors="coerce")

    if rule == "RAW":
        # Return raw rows (no resample). Keep needed columns.
        if by_region:
            res = tmp.reset_index()[[on, region_col] + metrics]
        else:
            res = tmp.reset_index()[[on] + metrics]
        return res

    if by_region:
        out = []
        for g, gdf in tmp.groupby(region_col):
            num = gdf[metrics].resample(rule).agg(agg_map)
            num[region_col] = g
            num = num.reset_index()
            out.append(num)
        res = pd.concat(out, ignore_index=True)
    else:
        res = tmp[metrics].resample(rule).agg(agg_map).reset_index()
    return res

# -----------------------------
# KPI
# -----------------------------
st.subheader("ğŸ“ˆ æ¦‚è¦KPI")
kc1, kc2, kc3, kc4 = st.columns(4)
with kc1:
    st.metric("ç·ä»¶æ•°", f"{len(fdf):,}")
with kc2:
    st.metric("æœŸé–“", f"{r[0].strftime('%Y-%m-%d')} ï½ {r[1].strftime('%Y-%m-%d')}")
with kc3:
    st.metric("åœ°åŸŸæ•°", f"{len(sel_regions)}")
with kc4:
    st.metric("é¸æŠæŒ‡æ¨™æ•°", f"{len(metric_cols)}")

# -----------------------------
# å¯è¦–åŒ–
# -----------------------------
st.subheader("ğŸ“Š å¯è¦–åŒ–")

if metric_cols:
    for m in metric_cols:
        st.markdown(f"**æ™‚ç³»åˆ—ï¼ˆ{m}ï¼‰**")
        ts = resample_frame(fdf, on=date_col, by_region=True, metrics=[m], freq=freq, how=agg_mode)
        chart = alt.Chart(ts).mark_line(point=True).encode(
            x=alt.X(f"{date_col}:T", title="æ—¥æ™‚"),
            y=alt.Y(f"{m}:Q", title=m),
            color=alt.Color(f"{region_col}:N", title="åœ°åŸŸ"),
            tooltip=[date_col, region_col, m]
        ).properties(height=300)
        st.altair_chart(chart, use_container_width=True)

    st.markdown("**åœ°åŸŸæ¯”è¼ƒï¼ˆæœŸé–“å†…ã®é›†è¨ˆå€¤ï¼‰**")
    agg_func = {"å¹³å‡":"mean","åˆè¨ˆ":"sum","ä¸­å¤®å€¤":"median"}[agg_mode]
    comp = fdf.groupby(region_col)[metric_cols].agg(agg_func).reset_index()
    chart = alt.Chart(comp.melt(id_vars=[region_col], var_name="é …ç›®", value_name="å€¤")).mark_bar().encode(
        x=alt.X("é …ç›®:N", title="é …ç›®"),
        y=alt.Y("å€¤:Q", title="å€¤"),
        color=alt.Color(f"{region_col}:N", title="åœ°åŸŸ"),
        column=alt.Column(f"{region_col}:N", title="åœ°åŸŸ")
    ).properties(height=280)
    st.altair_chart(chart, use_container_width=True)

    st.markdown("**åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰**")
    m = st.selectbox("ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®å¯¾è±¡åˆ—", options=metric_cols, index=0)
    series = fdf[m].dropna()
    if len(series) > 0:
        q5, q95 = np.nanpercentile(series, 5), np.nanpercentile(series, 95)
        hist_df = pd.DataFrame({m: series.clip(q5, q95)})
        chart = alt.Chart(hist_df).mark_bar().encode(
            x=alt.X(f"{m}:Q", bin=alt.Bin(maxbins=40), title=m),
            y=alt.Y("count():Q", title="ä»¶æ•°")
        ).properties(height=260)
        st.altair_chart(chart, use_container_width=True)
    else:
        st.info("ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¯¾è±¡ã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    st.markdown("**ãƒ”ãƒœãƒƒãƒˆï¼ˆåœ°åŸŸ Ã— æœˆï¼‰**")
    m = st.selectbox("ãƒ”ãƒœãƒƒãƒˆè¡¨ç¤ºã®å¯¾è±¡åˆ—", options=metric_cols, index=0, key="pivot_metric")
    tmp = fdf[[date_col, region_col, m]].dropna(subset=[date_col]).copy()
    tmp["æœˆ"] = tmp[date_col].dt.to_period("M").dt.to_timestamp()
    import numpy as _np
    agg = tmp.pivot_table(index=region_col, columns="æœˆ", values=m, aggfunc=_np.mean)
    st.dataframe(agg.style.format("{:,.2f}"), use_container_width=True)

# -----------------------------
# ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›
# -----------------------------
st.subheader("â¬‡ï¸ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›")
colx, coly = st.columns(2)
with colx:
    st.download_button(
        "ãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒ‡ãƒ¼ã‚¿ï¼ˆCSVï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=fdf.to_csv(index=False).encode("utf-8-sig"),
        file_name="filtered.csv",
        mime="text/csv"
    )
with coly:
    if metric_cols:
        t2 = fdf.copy()
        t2["æœˆ"] = t2[date_col].dt.to_period("M").dt.to_timestamp()
        out = t2.groupby(["æœˆ", region_col])[metric_cols].mean().reset_index().sort_values(["æœˆ", region_col])
        st.download_button(
            "æœˆæ¬¡é›†è¨ˆï¼ˆå¹³å‡, CSVï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=out.to_csv(index=False).encode("utf-8-sig"),
            file_name="monthly_summary_mean.csv",
            mime="text/csv"
        )

st.caption("Â© Streamlit app template for area bids by region (JP).")

# ---- Numeric-only hardening for metrics ----
def _coerce_numeric_columns(df_in: pd.DataFrame, cols: list[str]):
    safe_cols = []
    dropped_cols = []
    for c in cols:
        ser = pd.to_numeric(df_in[c].astype(str).str.replace(",", ""), errors="coerce")
        if ser.notna().any():
            df_in[c] = ser
            safe_cols.append(c)
        else:
            dropped_cols.append(c)
    return df_in, safe_cols, dropped_cols

